import random
import numpy as np
from tqdm.notebook import tqdm
import torch
import torchvision
from torch.utils.data import Dataset
from skimage import measure

class LeafGenerator(Dataset):

    '''
    Generates inputs and outputs for leaf tracer. Inputs are generated by subsampling
    tiles centered somewhere on the boundary of an object. An additional channel is 
    appended to the image tile with pixels belonging to the previously traced path.
    The output is a series of x and y pixel displacements corresponding to the next
    pixels along the contour of the leaf.
    
    Args: 
        images      (list): float arrays containing normalized RGB images
        masks       (list): bool arrays containing masks
        window_size  (int): width of the CNN input (e.g., 256)
        path_length  (int): num. pixels to predict in trace (e.g., 128)
        step_length  (int): num. previously traced pixels for context (e.g., 64)
        augment     (bool): whether to augment generated tiles
        verbose     (bool): whether to update user during initialization
        
    Returns:
        tile      (tensor): input tile for tracing CNN [3+1, W, W]
        new_trace (tensor): output ground truth trace [2, path_length]
    '''
    
    def __init__(
        self, 
        images, 
        masks,  
        window_size=256, 
        path_length=128,
        step_length=64,
        augment=False,
        verbose=False):
        
        # initialize
        super().__init__()
        self.images = images
        self.masks = masks
        self.window_size = window_size
        self.path_length = path_length
        self.step_length = step_length
        self.augment = augment
        self.verbose = verbose
        
        # extract contour indices from masks [N, 3]
        self.indices = self.extract_locations(masks)
        
        # define row/col pixel indices for sampling rotated tiles
        w = int(self.window_size/2)
        X, Y = np.meshgrid(np.arange(-w, w), np.arange(-w, w), indexing='ij')
        self.image_slice = np.concatenate([X.reshape(-1, 1), Y.reshape(-1, 1)], axis=1)
        
    def __len__(self):
        return len(self.indices)

    # get contour pixel locations [img_idx, row, col]
    def extract_locations(self, masks):
        
        # loop over each mask
        indices = []
        if self.verbose:
            print('Extracting contours...')
        mask_range = tqdm(range(len(masks))) if self.verbose else range(len(masks))
        for i in mask_range:
            
            # get mask contour
            contour = measure.find_contours(masks[i], 0.5) # [N, 2]
            lengths = [len(c) for c in contour]
            contour = contour[np.argmax(lengths)]
            contour = np.fliplr(contour) # (col, row)
            
            # include image indices
            contour = np.concatenate([i*np.ones_like(contour[:,0:1]), contour], axis=1) # [N, 3]
            indices.append(contour)
            
        # combine and shuffle
        indices = np.concatenate(indices, axis=0)
        indices = indices[np.random.permutation(len(indices))].astype(int)
            
        return indices
    
    # swap image axes [H, W, C] -> [C, H, W]
    def channels_first(self, image):
        return np.swapaxes(np.swapaxes(image, 0, 2), 1, 2)
    
    # swap image axes [C, H, W] -> [H, W, C]
    def channels_last(self, image):
        return np.swapaxes(np.swapaxes(image, 0, 2), 0, 1)
    
    # convert numpy image to torch tensor
    def image2torch(self, image):
        image = torch.tensor(self.channels_first(image), dtype=torch.float32) # [3, H, W]
        return image
    
    # convert numpy mask to torch tensor
    def mask2torch(self, mask):
        mask = torch.tensor(mask, dtype=torch.float32) # [H, W]
        return mask
    
    # convert torch tensor to numpy image
    def image2numpy(self, image):
        image = self.channels_last(image[:3].detach().cpu().numpy()) # [H, W, 3]
        return image
    
    # convert torch tensor to numpy mask
    def mask2numpy(self, mask):
        mask = mask.detach().cpu().numpy() # [H, W]
        return mask
    
    # extract ordered contour from boolean segmentation mask
    def extract_contour(self, mask):

        # extract contour(s)
        contours = measure.find_contours(mask, 0.5)
        
        # pre-process
        contours = [c for c in contours if len(c) >= 2*self.path_length]

        # if only one contour, done
        if len(contours) == 1: 
            contours = contours[0]
        
        # otherwise choose contour closest to tile center
        else: 
            window_size = mask.shape[0]
            dists_from_midpoint = []
            for i, contour in enumerate(contours):
                diffs = contour - np.array([[window_size/2,window_size/2]])
                dists = np.linalg.norm(diffs, ord=2, axis=-1)
                dists_from_midpoint.append(dists.min())
            contours = contours[np.argsort(dists_from_midpoint)[0]] 
        contours = np.fliplr(contours) # (col, row)

        return contours
    
    # find index of contour pixel closest to tile center
    def extract_midpoint(self, contours):
        midpoint = np.array([[self.window_size/2, self.window_size/2]])
        diffs = contours - midpoint
        dists = np.linalg.norm(diffs, ord=2, axis=-1)
        return np.argsort(dists)[0]
    
    # split contour into previous traced path and predicted path
    def split_contour(self, contour):
        midpoint_idx = self.extract_midpoint(contour)
        old_trace = contour[:midpoint_idx+1]
        new_trace = contour[midpoint_idx+1:]
        return old_trace, new_trace

    # draw previously traced contour onto new image channel
    def contour2channel(self, contour, mask):
        contour_channel = torch.zeros_like(mask)
        contour_channel[np.round(contour[:, 1]).astype(int), 
                        np.round(contour[:, 0]).astype(int)] = 1
        return contour_channel
    
    # builds rotation matrix from angle theta
    def build_rotation_matrix(self, theta):
        return np.array([
            [np.cos(theta), -np.sin(theta)], 
            [np.sin(theta),  np.cos(theta)]])
    
    # get rgb+1 tile and [2, m] contour
    def __getitem__(self, index, debug=False):
            
        # parse index
        idx = self.indices[index]
        i, k, j = idx[0], idx[1], idx[2] # image_idx, col, row
        
        # optionally jitter position
        if self.augment:
            j += int(np.random.normal(loc=0, scale=3))
            k += int(np.random.normal(loc=0, scale=3))
            
        # optionally apply rotation
        theta = np.random.choice(np.linspace(0, 2*np.pi, 360)) if self.augment else 0
        rotation = self.build_rotation_matrix(theta)
        locs = (np.array([[j, k]]) + np.dot(rotation, self.image_slice.T).T).astype(int)
        
        # extract tiles by (rotated) pixel locations, convert to torch
        w = self.window_size
        tile = self.images[i][locs[:,0], locs[:,1]].reshape(w, w, 3)
        mask = self.masks[i][locs[:,0], locs[:,1]].reshape(w, w)
        tile = self.image2torch(tile)
        mask = self.mask2torch(mask)

        # augment tiles
        if self.augment:
        
            # random horizontal/vertical flips
            if random.random() > 0.5:
                tile = torchvision.transforms.functional.hflip(tile)
                mask = torchvision.transforms.functional.hflip(mask)
            if random.random() > 0.5:
                tile = torchvision.transforms.functional.vflip(tile)
                mask = torchvision.transforms.functional.vflip(mask)

            # random color augmentation
            tile = torchvision.transforms.ColorJitter(
                brightness=0.5,
                contrast=0.5,
                hue=0.1, 
                saturation=0.1)(tile)

        # extract contour pixels from mask
        contour = self.extract_contour(mask.detach().cpu().numpy())
        old_trace, new_trace = self.split_contour(contour)
        
        # sample evenly from contour pixels
        old_idx = np.round(np.linspace(0, len(old_trace)-1, self.path_length)).astype(int)
        old_trace = old_trace[old_idx]
        new_idx = np.round(np.linspace(0, len(new_trace)-1, self.path_length)).astype(int)
        new_trace = new_trace[new_idx]
        
        # only include up to step length of previous trace
        old_trace = old_trace[-self.step_length:]

        # draw previously traced contour onto new image channel
        contour_channel = self.contour2channel(old_trace, mask)
        tile = torch.cat([tile, contour_channel[None]], dim=0)

        # center predicted contour about midpoint
        new_trace[:,0] = new_trace[:,0] - self.window_size/2
        new_trace[:,1] = self.window_size/2 - new_trace[:,1]
        new_trace = new_trace.T # [2, m]
        new_trace = torch.tensor(new_trace.copy(), dtype=torch.float32)
            
        return tile, new_trace