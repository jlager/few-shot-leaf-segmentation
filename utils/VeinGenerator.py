import random
import numpy as np
import torch
import torchvision
from torch.utils.data import Dataset
from scipy import ndimage

class VeinGenerator(Dataset):

    '''
    Generates inputs and outputs for vein grower. Inputs are generated by subsampling
    tiles centered somewhere inside the ROI of an object. The output is a 3 x 3 tile
    of pixel classifications to vein or background corresponding to pixels in the center
    of the tile.
    
    Args: 
        images      (list): float arrays containing normalized RGB images
        masks       (list): bool arrays containing masks
        rois        (list): bool arrays containing leaf segmentations
        window_size  (int): width of the CNN input (e.g., 128)
        augment     (bool): whether to augment generated tiles
        dilate       (int): number of pixels to expand the ROI
        
    Returns:
        tile      (tensor): input tile for tracing CNN [4, W, W]
        new_trace (tensor): output ground truth trace [2, path_length]
    '''
    
    def __init__(
        self, 
        images, 
        masks, 
        rois=None, 
        window_size=128, 
        augment=False,
        dilate=50):
        
        # initialize
        super().__init__()
        self.images = images
        self.masks = masks
        self.rois = rois
        self.window_size = window_size
        self.augment = augment
        self.dilate = dilate
        
        # break masks into foreground/background indices
        self.indices = self.extract_locations(masks, rois)
        
        # define row/col locations for sampling rotated tiles
        w = int(self.window_size/2)
        X, Y = np.meshgrid(np.arange(-w, w), np.arange(-w, w), indexing='ij')
        self.image_slice = np.concatenate([X.reshape(-1, 1), Y.reshape(-1, 1)], axis=1)
        
    def __len__(self):
        return len(self.indices)
    
    # swap image axes [H, W, C] -> [C, H, W]
    def channels_first(self, image):
        return np.swapaxes(np.swapaxes(image, 0, 2), 1, 2)
    
    # swap image axes [C, H, W] -> [H, W, C]
    def channels_last(self, image):
        return np.swapaxes(np.swapaxes(image, 0, 2), 0, 1)
    
    # convert numpy image to torch tensor
    def image2torch(self, image):
        image = torch.tensor(self.channels_first(image), dtype=torch.float32) # [3, H, W]
        return image
    
    # convert numpy mask to torch tensor
    def mask2torch(self, mask):
        mask = np.concatenate([mask[None], np.abs(-(mask[None]-1))], axis=0)
        mask = torch.tensor(mask, dtype=torch.float32) # [2, H, W]
        return mask
    
    # convert torch tensor to numpy image
    def image2numpy(self, image):
        image = self.channels_last(image.detach().cpu().numpy()) # [H, W, 3]
        return image
    
    # convert torch tensor to numpy mask
    def mask2numpy(self, mask):
        mask = mask[0].detach().cpu().numpy() # [H, W]
        return mask
    
    # build rotation matrix based on angle (in radians)
    def build_rotation_matrix(self, theta):
        return np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])
    
    # get foreground and background pixel locations
    def extract_locations(self, masks, rois=None):
        
        # loop over each mask
        w = int(self.window_size/2)
        indices = []
        for i in range(len(masks)):
            
            # get foreground/background masks
            f_mask = masks[i][w:-w,w:-w].copy()
            b_mask = (1 - masks[i][w:-w,w:-w]).copy()
            
            # apply ROI if applicable
            if rois is not None:

                # account for padding
                roi = rois[i][w:-w,w:-w]

                # optionally expand ROI
                if self.dilate > 0: 
                    roi = ndimage.binary_dilation(
                        input=roi.astype(bool), 
                        iterations=self.dilate).astype(float)

                # apply ROI
                f_mask = f_mask * roi
                b_mask = b_mask * roi
            
            # convert mask to index
            f_idx = np.array(np.where(f_mask)).T + w # [F, 2]
            b_idx = np.array(np.where(b_mask)).T + w # [B, 2]
            
            # randomly subsample background locations
            m = min(10*len(f_idx), len(b_idx))
            b_idx = b_idx[np.random.permutation(len(b_idx))[:m]] # [N*F, 2]
            
            # combine and include image indices
            idx = np.concatenate([f_idx, b_idx], axis=0) # [(N+1)*F, 2]
            idx = np.concatenate([i*np.ones_like(idx[:,0:1]), idx], axis=1) # [(N+1)*F, 3]
            indices.append(idx)
        
        # shuffle
        indices = np.concatenate(indices, axis=0)
        indices = indices[np.random.permutation(len(indices))].astype(int)
        
        return indices
    
    def __getitem__(self, index):
        
        # get index
        idx = self.indices[index]
        i, j, k = idx[0], idx[1], idx[2]
        
        # optionally apply rotation
        theta = np.random.choice(np.linspace(0, 2*np.pi, 360)) if self.augment else 0
        rotation = self.build_rotation_matrix(theta)
        locs = (np.array([[j, k]]) + np.round(np.dot(rotation, self.image_slice.T).T)).astype(int)
        
        # extract tiles by (rotated) pixel locations, convert to torch
        w = self.window_size
        tile = self.images[i][locs[:,0], locs[:,1]].reshape(w, w, 3)
        mask = self.masks[i][locs[:,0], locs[:,1]].reshape(w, w)
        tile = self.image2torch(tile.astype(np.float32))
        mask = self.mask2torch(np.round(mask.astype(np.float32)))
        
        if self.augment:
        
            # random flips
            if random.random() > 0.5:
                tile = torchvision.transforms.functional.hflip(tile)
                mask = torchvision.transforms.functional.hflip(mask)
            if random.random() > 0.5:
                tile = torchvision.transforms.functional.vflip(tile)
                mask = torchvision.transforms.functional.vflip(mask)

            # random color augmentation
            tile = torchvision.transforms.ColorJitter(
                brightness=0.5,
                contrast=0.5,
                hue=0.1, 
                saturation=0.1)(tile)
            
            # random blur
            kernel = np.random.choice([2*i+1 for i in range(5)])
            tile = torchvision.transforms.GaussianBlur(
                kernel_size=kernel, sigma=(0.1, 1))(tile)
            
            # clip
            tile = tile.clamp(0, 1)
            
        # zoom into center of mask
        w = int(self.window_size/2)
        mask = mask[:, w-1:w+2, w-1:w+2]
        
        return tile, mask